````markdown
# RedditÂ â†’Â SaaSÂ IdeaÂ FinderÂ ðŸª„ðŸš€

A oneâ€‘stop toolkit that:

1. **Scrapes** Reddit posts & comments from subreddits you choose  
2. **Filters** for *real*, *softwareâ€‘solvable* painâ€‘points with GPTâ€‘4oâ€‘mini  
3. **Designs** a 1â€‘toâ€‘3â€‘feature MVP for each viable problem (no generic CRUD apps!)  
4. **Generates** a stepâ€‘byâ€‘step **Cursor playbook** so you can build the MVP at lightning speed  
5. Lets you explore everything via a **Streamlit GUI** or an easy CLI

---

## Key Components

| File | Role |
|------|------|
| **`standalone_reddit_scraper.py`** | Selfâ€‘contained CLI & library. Houses the whole scrapeâ€¯â†’â€¯GPT pipeline and writes results to `results.jsonl`. |
| **`gui_reddit_scraper.py`** (or `gui_for_main.py`) | Streamlit frontâ€‘end: pick subs, kick off scrapes, browse & download ideas. |
| **`scraper.db`** | SQLite store that remembers which Reddit submissions youâ€™ve already processedâ€”prevents duplicate analysis. |
| **`results.jsonl`** | Appendâ€‘only ledger of every viable idea, its solution proposal, and the Cursor playbook. Each line is a complete JSON record. |
| **`.env`** | Holds credentials for Reddit & OpenAI (kept out of Git). |

---

## Features

- **ViabilityÂ analysis** â€“ GPT decides if a discussion exposes a genuine, paidâ€‘worthy problem.
- **Contextâ€‘rich solutions** â€“ target market + post excerpt pushed into the prompt; strong guardrail forbids generic task managers.
- **Cursor playbook** â€“ numbered prompts that you can paste into Cursorâ€™s chat to scaffold the entire project (repoâ€¯â†’â€¯schemaâ€¯â†’â€¯endpointsâ€¯â†’â€¯testsâ€¯â†’â€¯deploy).
- **Duplicate protection** â€“ the scraper wonâ€™t spend tokens on a post youâ€™ve handled before.
- **Two launch modes**  
  * **CLI** â€“ great for cron jobs or data batches  
  * **GUI** â€“ tweak subreddits, watch ideas stream in, download JSON right in the browser.

---

## Installation

```bash
git clone <thisâ€‘repo>
cd Redditâ€‘SaaSâ€‘Ideaâ€‘Finder
python -m venv venv && source venv/bin/activate   # Windows: .\venv\Scripts\activate
pip install -r requirements.txt                   # or pip install praw openai python-dotenv streamlit pandas
````

Create a **`.env`** at the project root:

```
REDDIT_CLIENT_ID=xxxx
REDDIT_CLIENT_SECRET=yyyy
REDDIT_USER_AGENT=RedditIdeaFinder/0.3
OPENAI_API_KEY=sk-...
OPENAI_ORG=org_...        # optional
```

---

## QuickÂ StartÂ â€”Â CLI

```bash
python standalone_reddit_scraper.py \
  --subreddits consulting startups ChatGPT \
  --posts-per-subreddit 5 \
  --comments-per-post 20 \
  --output results.jsonl
```

*New viable ideas append to `results.jsonl`; duplicates are skipped automatically.*

---

## QuickÂ StartÂ â€”Â GUI

```bash
streamlit run gui_for_main.py
```

* Open [http://localhost:8501](http://localhost:8501)
* Enter commaâ€‘separated subreddits, tweak sliders, hit **â€œðŸš€Â Scrape nowâ€**
* Table updates live; click **Download JSONL** any time.

---

## Result Schema

```jsonc
{
  "meta":   { "uuid": "...", "scraped_at": "2025â€‘07â€‘13T19:31:17Z" },
  "reddit": { "subreddit": "startups", "url": "...", "title": "Feedback Friday" },
  "analysis": {
    "is_viable": true,
    "problem_description": "...",
    "target_market": "...",
    "confidence_score": 0.83
  },
  "solution": {
    "solution_description": "...",
    "tech_stack": ["Next.js", "Supabase"],
    "mvp_features": ["FeatureÂ A", "FeatureÂ B"],
    "est_development_time": "10â€“14Â days"
  },
  "cursor_playbook": [
    "PromptÂ 1 â€“Â create repoâ€¦",
    "PromptÂ 2 â€“Â design schemaâ€¦",
    "â€¦"
  ]
}
```

---

## Customising

* **Change model / temperature** â€“ edit `MODEL` or `TEMPERATURE` constants in `standalone_reddit_scraper.py`.
* **Different sorting** â€“ in `scrape_subreddit()` switch `.hot()` to `.new()` or `.top(time_filter="day")`.
* **Longer context** â€“ raise `context` slice length (default 1â€¯200Â chars) passed to `SOLUTION_PROMPT`.
* **Different storage** â€“ swap SQLite functions in `gui_reddit_scraper.py` for Postgres, Supabase, etc.

---

## Troubleshooting

| Problem                                                                    | Fix                                                                                                                                                                                  |
| -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `AttributeError: module 'streamlit' has no attribute 'experimental_rerun'` | Youâ€™re on an older Streamlit. Keep the fallback block in the GUI or run `pip install --upgrade streamlit`.                                                                           |
| Duplicate ideas still appear                                               | `scraper.db` only deâ€‘dupes by Reddit post ID. If you scrape with *new* sort order, IDs are unique and no dupe should appear; otherwise bump the `post_limit` or adjust the DB logic. |
| Costs creeping up                                                          | Lower `posts-per-subreddit`, shorten `context`, or switch to `gptâ€‘3.5â€‘turbo` in the `MODEL` constant.                                                                                |

---

## License

MITÂ â€“Â do whatever you want, just donâ€™t blame us if you ship an MVP that becomes a unicorn. ðŸ¦„

---

> Made with â˜•, GPTâ€‘4oâ€‘mini, and a little vibeâ€‘coding magic.

### Local dev

```bash
pip install -r requirements.txt
# add secrets in .streamlit/secrets.toml
```

```
```
